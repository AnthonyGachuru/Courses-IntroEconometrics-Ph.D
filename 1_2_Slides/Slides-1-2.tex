%%!TEX TS-program = latex

%This template gives a nice gray-sober environment. 
\documentclass[red,handout]{beamer}
\usepackage{etex}
\usepackage[utf8]{inputenc}
\input ../Auxiliary_Format_Files/PreambleSlides.tex
 
\title{{\scshape Econometrics I}}
\author{{\scshape Jos\'e Luis Montiel Olea}}
\date{}





%---------------------------------------------------Begin Document-------------------------------------
\begin{document}
\setbeamerfont{alerted text}{series=\normalfont}
\setbeamercolor{alerted text}{fg=blue}

\frame{\titlepage}


\frame{
\begin{center}
Introduction to Probability and Statistics for Economists \\
(Ph.D. in Economics, 1st year)
\end{center}

}



\frame{
\begin{center}
\textbf{Lectures 1 and 2} 
\end{center}
}

\section{Introduction}

\frame{
\frametitle{\normalsize \scshape Two Main Blocks}
\justifying
\begin{enumerate}
\item \alt<2>{\textcolor{blue}{{\scshape Probability Theory}}}{{\scshape Probability Theory}} \\
\vspace{.3cm}
\begin{center}\invisible<1-2>{\textcolor{blue}{Basic tools to model uncertainty}}
\end{center}
\vspace{.2cm}
\item \alt<4>{\textcolor{blue}{{\scshape Mathematical Statistics}}}{{\scshape Mathematical Statistics }} \\
\vspace{.3cm}
\begin{center}
\invisible<1-4>{\textcolor{blue}{Data $\rightarrow$ Decisions}}
\end{center}
\only<5>{}
\end{enumerate}
}

\section{Probability Space/Random Variables}

\frame{
\begin{center}
\textbf{Probability Theory}
\end{center}
\begin{center}
How to model `randomness'?
\end{center}
}


\frame{
\frametitle{\normalsize {\scshape Things we will learn in Lecture 1:}}
\begin{itemize} [<alert@+>]
\item [$\star$] What is a probability space? \vspace{.4cm}
\begin{enumerate}
\item What is measurable space? \vspace{.3cm}
\item What is a probability space? \vspace{.4cm}
\end{enumerate}
\item [$\star$] What is a random variable? \vspace{.4cm}
\item [$\star$] What is the `distribution’ or `law’ of a random variable? 
\end{itemize}
}

\frame{
\begin{center}
\textbf{What is a probability space?}
\end{center}
$$(\Omega, \mathcal{F}, \prob)  $$
}

\frame{
\frametitle{\normalsize {\scshape Probability Space: Two components}}
$$(\Omega, \mathcal{F}, \prob)  $$
\begin{enumerate} [<+- | alert@+>]
\item $(\Omega, \mathcal{F})$ measurable space. 
\item [] $$\mathcal{F}: \textrm{ Set of events } (\subseteq \Omega).$$
\item $\prob: \mathcal{F} \rightarrow [0,1].$ Probability Measure
\item [] $$\text{`How likely is an event in $\mathcal{F}$'}$$ 
\end{enumerate}
}

\frame{
\frametitle{\normalsize \caps{Measurable Space}}
\begin{center}
See notes. 
\end{center}
}


\frame{
\frametitle{\normalsize {\scshape Probability Measures}} 
\begin{enumerate} [<+-|alert@+>]
\item [$\star$] $\prob(\phi)=0, \: \prob(\Omega)=1$ (Normalization) \\ \vspace{.5cm}
\item [$\star$] For any finite collection $A_1, A_2, \ldots A_m$ such that $A_i \cap A_j = \emptyset$

$$\prob\Big( \cup_{i=1}^{m} A_i \Big) = \sum_{i=1}^{m} \prob(A_i) $$
\item [] This property is called \textcolor{blue}{additivity}. \vspace{.3cm}
\item [$\star$] If you replace finite by \emph{countably infinite}, Property 2 is called \textcolor{blue}{$\sigma$-additivity}.
\end{enumerate}
}

\frame{
\frametitle{\normalsize {\scshape Important}}
\begin{center}
\textcolor{blue}{Normalization and $\sigma$-additivity define a probability measure}
\end{center}
}





\frame{
\begin{center}
\textbf{What is a random variable?}
\end{center}
$$ X: \Omega \rightarrow S$$
}



\frame{
\frametitle{\normalsize {\scshape Random Variable}}
$$\textcolor{blue}{X: \Omega \rightarrow S} $$ \pause
\begin{enumerate}[<+- | alert@+>]
\item[$\star$] $\Omega:$ Set of states of the world.\\ \vspace{.5cm}
\item[$\star$] $S:$ Image Space
\vspace{.5cm}
\item[$\star$] $X:$ Random Variable \vspace{.5cm}
\end{enumerate}
}



\frame{
\begin{center}
\textbf{What is the distribution or law of a random variable?} 
\end{center}
}



\frame{
\frametitle{\normalsize {\scshape `Induced’ Probability of a Random Variable}}
\begin{enumerate}
\item [$\star$]  \alt<2>{\textcolor{blue}{The probability $\prob$ on $\Omega$ induces a probability on subsets of $S$:}}{The probability $\prob$ on $\Omega$ induces a probability on subsets of $S$:}

$$\prob_{X} [F]  \equiv \prob [ \: \{\omega \: | \: X(\omega) \in F \} \:  ], \quad F \subseteq S $$ \vspace{.2cm}

\item [$\star$] \alt<3>{\textcolor{blue}{How likely are the states of the world in which F occurs?}} {How likely are the states of the world in which F occurs?} 
\end{enumerate}
}


\frame{
\begin{center}
\textcolor{blue}{The induced probability of a random variable is usually called its {\scshape Distribution or Law}}
\end{center}
}



\frame{
\begin{center}
\textcolor{blue}{Different random variables can induce the same probability on $S$.}
\end{center}
}

\frame{
\frametitle{\normalsize \alt<2>{$P_{X_2}(1)=.5$}{$P_{X_1}(1)=.5$}}
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(-3,-3)(-3,-3)(3,3)
\uput[25](3.5,-3.4){\footnotesize $\Omega$}
\uput[25](-3.3,-3.4){\footnotesize $0$}
\uput[25](2.5,-3.4){\footnotesize $1$}
\uput[25](-3.7,3.2){\footnotesize $S=\{0,1\}$}
\only<1>{\psline[linecolor=red, linewidth=.1] (-3,2)(-.2,2)}
\only<1>{\psline[linecolor=red, linewidth=.1] (-.2,-3)(3,-3)}
\only<2>{\psline[linecolor=blue, linewidth=.1] (-3,-3)(-.2,-3)}
\only<2>{\psline[linecolor=blue, linewidth=.1] (-.2,2)(3,2)}
\uput[25](-.4,-3.4){\footnotesize $0.5$}
\end{pspicture*} 
\end{center}
}

\section{C.D.F.}

\frame{
\begin{center}
\textbf{The Cumulative Distribution Function of Real-valued Random Variables }
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape c.d.f of an $\R$-valued random variable}}
\begin{enumerate}
\item [$\star$] \alt<2>{\textcolor{blue}{How likely is a realization of the random variable X below x?}}{How likely is a realization of the random variable X below x?} \vspace{.3cm}

\item [$\star$] \alt<3>{\textcolor{blue}{The c.d.f. summarizes this information}}{The c.d.f. summarizes this information} 
$$F_{X}: \R \rightarrow [0,1] $$

$$F_{X}(x) \equiv \prob\Big\{ \omega \in \Omega \: | \: X(\omega) \leq x \Big\} $$
\end{enumerate}
}


\frame{
\begin{center}
\textbf{Examples of c.d.f.}
\end{center}
}

\frame{
\frametitle{\normalsize $X(\omega)=a + \omega [b-a]$}
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(-3,-3)(-3,-3)(3,3)
\uput[25](3.5,-3.4){\footnotesize $\Omega$}
\uput[25](-3.3,-3.4){\footnotesize $0$}
\uput[25](2.5,-3.4){\footnotesize $1$}
\uput[25](-3.7,3.2){\footnotesize $S=\R$}
\uput[25](-3.7,-3){\footnotesize $a$}
\uput[25](-3.7,2.8){\footnotesize $b$}
\psPolynomial[coeff=0 1,linecolor=red]{-3}{2.8}
\end{pspicture*} 
\end{center}
}

\frame{
If $x \in [a,b]:$
\begin{eqnarray*}
\prob\{ \omega \in \Omega \: | \: X(\omega) \leq x \} &=& \prob\{ \omega \in \Omega \: | \: a+\omega(b-a) \leq x \} \\ \pause
&=& \prob\{ \omega \in \Omega \: | \: \omega (b-a) \leq x-a  \} \\ \pause
&=& \prob\{ \: [0, x-a/(b-a)] \: \} \\ \pause
&=& x-a/(b-a) \pause
\end{eqnarray*}
\pause Hence,

$$F_{X}(x) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & x < a   \\
& &\\
(x-a)/b-a &  \text{if} & x \in [a,b)  \\
 & &\\
 1 & \text{if}  & x \geq b   
\end{array}
\right.
 $$

}


\frame{
\frametitle{\normalsize \alt<2>{\scshape Uniform Distribution on $[a,b]$}{c.d.f. of  $X(\omega)=a + \omega [b-a]$}}
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(0,-3)(-4,-3)(4,3)
\uput[25](3.5,-3.4){\footnotesize $\R$}
\uput[25](-.3,-3.4){\footnotesize $0$}
\uput[25](-.5,-1){\footnotesize $1$}
\uput[25](-.5,3.2){\footnotesize $F_X(\cdot)$}
\psline[linecolor=blue, linewidth=.1] (-4,-3)(-1,-3)
\psline[linecolor=blue, linewidth=.1] (1,-1)(4,-1)
\psPolynomial[coeff=-2 1, linewidth=.1, linecolor=blue]{-1}{1}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $X(\omega) = \mathbf{1}[\omega \geq 1-p ]$ }


\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(-3,-3)(-3,-3)(3,3)
\uput[25](3.5,-3.4){\footnotesize $\Omega$}
\uput[25](-3.3,-3.4){\footnotesize $0$}
\uput[25](2.5,-3.4){\footnotesize $1$}
\uput[25](-3.7,3.2){\footnotesize $S=\R$}
\psline[linecolor=red] (-3,-3)(-2,-3)
\psline[linecolor=red] (-2,2)(3,2)
\uput[25](-2.3,-3.4){\footnotesize $1-p$}

\end{pspicture*} 
\end{center}
}

\frame{
$$F_{X}(x) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & x < 0   \\
& &\\
1-p &  \text{if} & x \in [0,1)  \\
 & &\\
 1 & \text{if}  & x \geq 1  
\end{array}
\right.
 $$
}


\frame{
\frametitle{\normalsize \alt<2>{{\scshape Bernoulli Distribution with parameter $p$}}{c.d.f. of  $X(\omega)=\mathbf{1}[\omega \geq 1-p ]$}}
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(0,-3)(-4,-3)(4,3)
\uput[25](3.5,-3.4){\footnotesize $\R$}
\uput[25](-.3,-3.4){\footnotesize $0$}
\uput[25](-.5,-1){\footnotesize $1$}
\uput[25](2,-3.4){\footnotesize $1$}
\uput[25](-1,-2.7){\footnotesize $1-p$}
\uput[25](-.5,3.2){\footnotesize $F_X(\cdot)$}
\psline[linecolor=blue, linewidth=.1] (-4,-3)(0,-3)
\psline[linecolor=blue, linewidth=.1] (0,-2.5)(2.25,-2.5)
\psline[linecolor=blue, linewidth=.1] (2.25,-1)(4,-1)
\end{pspicture*} 
\end{center}
}


\frame{ 
\frametitle{\normalsize $X(\omega)=\omega^2$}
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(-3,-3)(-3,-3)(3,3)
\uput[25](3.5,-3.4){\footnotesize $\Omega$}
\uput[25](-3.3,-3.4){\footnotesize $0$}
\uput[25](2.5,-3.4){\footnotesize $1$}
\uput[25](-3.7,3.2){\footnotesize $S=\R$}
\uput[25](-3.7,-3){\footnotesize $0$}
\uput[25](-3.7,2.8){\footnotesize $1$}

\psPolynomial[coeff=-1.5 1 .166,linecolor=red]{-3}{2.8}
\end{pspicture*} 
\end{center}
}

\frame{
If $x \in [0,1]:$
\begin{eqnarray*}
\prob\{ \omega \in \Omega \: | \: X(\omega) \leq x \} &=& \prob\{ \omega \in \Omega \: | \: \omega^2 \leq x \} \\ \pause
&=& \prob\{ \omega \in \Omega \: | \: \omega \leq \sqrt{x}  \} \\ \pause
&=& \prob\{ \: [0, \sqrt{x}] \: \} \\ \pause
&=& \sqrt{x}
\end{eqnarray*}
\pause Hence,

$$F_{X}(x) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & x < 0   \\
& &\\
 \sqrt{x} &  \text{if} & x \in [0,1)  \\
 & &\\
 1 & \text{if}  & x \geq 1   
\end{array}
\right.
 $$

}

\section{Discrete/Continuous Type}

\frame{
\begin{center}
\textbf{Discrete and Continuous Type of Real-Valued Random Variables}
\end{center}
}

\frame{
\begin{center}
Let's take a look to the c.d.f.s we have computed
\end{center}
}

\frame{
\frametitle{\normalsize \scshape Uniform Distribution on $[a,b]$ }
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(0,-3)(-4,-3)(4,1.3)
\uput[25](-3.5,2){\footnotesize $F_{X}(x) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & x < a   \\
& &\\
(x-a)/b-a &  \text{if} & x \in [a,b)  \\
 & &\\
 1 & \text{if}  & x \geq b   
\end{array}
\right.
 $}
\uput[25](3.5,-3.4){\footnotesize $\R$}
\uput[25](-.3,-3.4){\footnotesize $0$}
\uput[25](-1.2,-3.4){\footnotesize $a$}
\uput[25](.8,-3.4){\footnotesize $b$}
\uput[25](-.5,-1){\footnotesize $1$}
\psline[linecolor=blue, linewidth=.1] (-4,-3)(-1,-3)
\psline[linecolor=blue, linewidth=.1] (1,-1)(4,-1)
\psPolynomial[coeff=-2 1, linewidth=.1, linecolor=blue]{-1}{1}
\end{pspicture*} 
\end{center}
}



\frame{
\frametitle{\normalsize {\scshape Bernoulli Distribution with parameter $p$} }
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(0,-3)(-4,-3)(4,1.3)
\uput[25](-3,2){\footnotesize $F_{X}(x) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & x < 0   \\
& &\\
1-p &  \text{if} & x \in [0,1)  \\
 & &\\
 1 & \text{if}  & x \geq 1  
\end{array}
\right.
 $}
\uput[25](3.5,-3.4){\footnotesize $\R$}
\uput[25](-.3,-3.4){\footnotesize $0$}
\uput[25](-.5,-1){\footnotesize $1$}
\uput[25](2,-3.4){\footnotesize $1$}
\uput[25](-1,-2.7){\footnotesize $1-p$}
\psline[linecolor=blue, linewidth=.1] (-4,-3)(0,-3)
\psline[linecolor=blue, linewidth=.1] (0,-2.5)(2.25,-2.5)
\psline[linecolor=blue, linewidth=.1] (2.25,-1)(4,-1)
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape What are the common properties?}}

\begin{enumerate}
\item $F_{X}$ is non-decreasing \vspace{.3cm}
\item $\lim_{x \rightarrow \infty} F_{X}(x) =1$ \vspace{.3cm}
\item $\lim_{x \rightarrow -\infty} F_{X}(x) =0$ \vspace{.3cm}
\item $\lim_{h \rightarrow 0^{+}} F_{X}(x+h) = F_{X}(x)$ \vspace{.3cm} \pause
\end{enumerate}
\begin{center}
\item [] \textcolor{blue}{In fact, these 4 properties characterize the induced c.d.f. of a real-valued random variable!}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Discrete Distributions / Discrete r.v.s }} 
{\scshape Distribution:} Disributions for which 
$\exists$ a countable set 
$$\text{Supp}=\{x_1, x_2, \ldots\}, x_i \in \R,$$ 
such that \vspace{.3cm}
\begin{enumerate}[a)] 
\item $\prob_{X}(X=x_i)>0 \quad \forall \quad x_i \in \text{Supp}$ \vspace{.2cm}
\item  $\sum_{x_i \in \text{Supp}} \prob_{X}(X=x_i) = 1$
\end{enumerate} \vspace{.3cm}
are called \textcolor{blue}{discrete}. 
}

\frame{
\frametitle{\normalsize {\scshape p.m.f.}}
\begin{center}
\alt<2>{\textcolor{blue}{We will identify discrete distributions/r.v.s by its support and its p.m.f.}}{The function $\prob_{X}(X=x_i)$ is called the \textcolor{blue}{Probability Mass Function} (p.m.f.)}
\end{center}
}


\frame{
\frametitle{\normalsize {\scshape Uniform[a,b]}}
\begin{itemize}[<+->]
\item [$\star$] Note that the $U[a,b]$ is not \textcolor{blue}{discrete}. Why? \vspace{.3cm} 
\item [$\star$] $P_{X}(X=x) = 0 \quad \forall \quad x \in \R.$ \vspace{.3cm} 
\item [$\star$] However, $U[a,b]$ has a special property as well! \vspace{.3cm} 

\item []
$$\textcolor{blue}{ F_X(x) = \int_{-\infty}^{x} \frac{1}{b-a} \mathbf{1}\{z \in [a,b]\} dz   } $$

\end{itemize}
}

\frame{
\frametitle{\normalsize {\scshape (Absolutely) Continuous Distributions}}
Random Variables for which
$$ \textcolor{blue}{ F_X(x) = \int_{-\infty}^{x} f(z) dz   }  $$
\noindent for some $f(z) \geq 0$ $\forall z \in \R$ are called: \pause \vspace{.3cm}
\begin{center}
\textcolor{blue}{(Absolutely) Continuous} \pause
\end{center} 
The function $f(z)$ is called \pause \vspace{.3cm}
\begin{center}
\textcolor{blue}{Probability Density Function (p.d.f.)} 
\end{center}
}

\frame{
\begin{center}
\begin{center}
\textcolor{blue}{We will identify continuous distributions/r.v.s by its p.d.f.}
\end{center}
\end{center}
}


\frame{
\begin{center}
\textbf{Examples of (Univariate) Discrete Distributions}
\end{center}
}


\frame{
\begin{center}
\textbf{Discrete Finite Support}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Bernoulli Distribution $(p)$,  $p \in (0,1)$}}
\begin{itemize}
\item [$\star$] The Bernoulli distribution with parameters $p$ has support:
$$\text{Supp}=\{0, 1\} $$ \pause
and p.m.f. given by: \pause
$$\prob_{X}(X=x)=p^{x} (1-p)^{1-x} \quad x \in \{0,1\}$$
\end{itemize}
}

\frame{
\frametitle{\normalsize {\scshape How do we know it is a p.m.f.?}}
Two parts: \vspace{.3cm}
\begin{enumerate}[a)]
\item $\prob_{X}(X=x)>0 \quad \forall \: x \in \{0,1\}$. Easy to verify:
$$\textcolor{blue}{p^{x} (1-p)^{1-x}>0}$$ \pause
\item 
$$\sum_{x \in \{0,1\}} p^{x} (1-p)^{1-x} = (1-p) + p$$
\end{enumerate}
}


\frame{
\frametitle{\normalsize {\scshape Binomial Distribution $(n,p)$, $n \in \mathbb{N}, p \in (0,1)$}}
\begin{itemize}
\item [$\star$] The binomial distribution with parameters $(n,p)$ has support:
$$\text{Supp}=\{0,1, 2, \ldots n\} $$ \pause
and p.m.f. given by: \pause
$$\prob_{X}(X=x) \equiv \frac{n!}{(n-x)! x!} p^{x} (1-p)^{n-x} \quad x \in \text{Supp}$$
\end{itemize}
}

\frame{
\frametitle{\normalsize {\scshape How do we know it is a p.m.f.?}}
Two parts: \vspace{.3cm}
\begin{enumerate}[a)]
\item $\prob_{X}(X=x)>0 \quad \forall \: x \in \{0,1,2, \ldots n\}$. Easy to verify:
$$\textcolor{blue}{\frac{n!}{(n-x)! x!} p^{x} (1-p)^{n-x}>0}$$ \pause
\item 
$$\sum_{x \in \{0,1, \ldots n\}} \frac{n!}{(n-x)! x!} p^{x} (1-p)^{n-x} =1 ?$$
\end{enumerate}
}

\frame{
\frametitle{\normalsize {\scshape How do we know it is a p.m.f.?}}
Use the Binomial Theorem
$$(a+b)^n = \sum_{x \in \{0,1, \ldots n\}} \frac{n!}{(n-x)! x!} a^{x} b^{n-x} =1$$ \pause
$$a=p, \quad b=1-p $$
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.1)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(0,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.1}
\end{pspicture}
\end{center}
}


\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.3)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.3}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.5)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.5}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.7)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.7}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.9)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.9}
\end{pspicture}
\end{center}
}


\frame{
\frametitle{\normalsize {\scshape Binomial $(5,.1)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(6,0)
\uput[-90](6,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{5}{0.1}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(5,.3)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(6,0)
\uput[-90](6,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{5}{0.3}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(5,.5)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(6,0)
\uput[-90](6,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{5}{0.5}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(5,.7)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(6,0)
\uput[-90](6,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{5}{0.7}
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(5,.9)$ }}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(6,0)
\uput[-90](6,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{5}{0.9}
\end{pspicture}
\end{center}
}


\frame{
\begin{center}
\textbf{Examples of (Univariate) Continuous Distributions}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape (Absolutely) Continuous Real-valued r.v.s}}
Random Variables for which
$$ \textcolor{blue}{ F_X(x) = \int_{-\infty}^{x} f(z) dz   }  $$
\noindent for some $f(z) \geq 0$ $\forall z \in \R$ are called: \pause \vspace{.3cm}
\begin{center}
\textcolor{blue}{(Absolutely) Continuous} 
\end{center} 
The function $f(z)$ is called  \vspace{.3cm}
\begin{center}
\textcolor{blue}{Probability Density Function (p.d.f.)} 
\end{center}
}

\frame{ 
\frametitle{\normalsize {\scshape Properties of a p.d.f.}}
\begin{itemize}
\item [$\star$] A function $f:\R \rightarrow \R$ is a p.d.f. if \vspace{.5cm}
\begin{enumerate} [a)]
\item $f(z) \geq 0$ \vspace{.25cm}
\item $\int_{\{z \in \R \: | \: f(z) > 0 \}} f(z) dz = 1$ \vspace{.5cm}
\end{enumerate}   
\item [$\star$] a), b) $\implies$ $F(x) = \int_{-\infty}^{x} f(z)dz$ is a c.d.f. \vspace{.5cm}
\item[$\star$] The set
$$\{z \in \R \: | \: f(z) > 0 \} \subseteq \R $$
is called the \textcolor{blue}{support} of the continuous r.v. \vspace{.5cm}
\end{itemize}
}

\frame{
\frametitle{\normalsize {\scshape From p.d.f. to probabilities}}
Let $X$ be a real-valued random variable with p.d.f. $f(z)$ \pause \vspace{.5cm}
\begin{itemize}  [<+->]
\item [$\star$] \alert<2>{$\prob_{X}[X \leq a] = \int_{-\infty}^{a} f(z) dz$} \vspace{.5cm}
\item [$\star$] \alert<3>{$\prob_{X}[a \leq X \leq b] = \int_{a}^{b} f(z) dz$} \vspace{.5cm} 
\item [$\star$] \alert<4>{$\prob_{X}[X > a ] = \int_{a}^{\infty} f(z) dz$} 
\end{itemize}
}


\frame{
\begin{center}
\textbf{Examples of Absolutely Continuous Distributions}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Uniform [a,b]}}
\begin{itemize}
\item [$\star$] The uniform distribution with parameters $[a,b]$ has p.d.f. 
$$f(z)=\frac{1}{b-a} \mathbf{1}\{z \in [a,b]\}$$ \pause 
\item [$\star$] and support $[a,b]$
\end{itemize}
}

\frame{
\frametitle{\normalsize \scshape p.d.f. of the Uniform Distribution on $[a,b]$ }
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(0,-3)(-4,-3)(4,1.3)
\uput[25](-3.5,2){\footnotesize $f(z) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & z < a   \\
& &\\
1/(b-a) &  \text{if} & z \in [a,b]  \\
 & &\\
 0 & \text{if}  & z > b   
\end{array}
\right.
 $}
\uput[25](3.5,-3.4){\footnotesize $\R$}
\uput[25](-.3,-3.4){\footnotesize $0$}
\uput[25](-1.2,-3.4){\footnotesize $a$}
\uput[25](.8,-3.4){\footnotesize $b$}
\uput[25](1.2,-1.1){\footnotesize $1/(b-a)$}
\psline[linecolor=blue, linewidth=.1] (-4,-3)(-1,-3)
\psline[linecolor=blue, linewidth=.1] (1,-3)(4,-3)
\psPolynomial[coeff=-1 0, linewidth=.1, linecolor=blue]{-1}{1}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Normal Distribution $(\mu, \sigma^2)$}}
\begin{itemize}
\item [$\star$] The normal distribution with parameters $(\mu, \sigma^2)$ has p.d.f. 
$$f(z)=\frac{1}{\sigma \sqrt{2 \pi}} \exp\Big( -\frac{1}{2 \sigma^2} (z-\mu)^2 \Big)$$ \pause 
\item [$\star$] and support $\R$
\end{itemize}
}

\frame{
\frametitle{\normalsize {\scshape Normal Distribution $(\mu, \sigma^2)$}}
\begin{itemize}
\item [$\star$] $f(z)> 0 \pause \checkmark$ \pause \vspace{.5cm}
\item [$\star$] How do we know that:
$$ \int_{-\infty}^{\infty} \frac{1}{\sigma \sqrt{2 \pi}} \exp\Big( -\frac{1}{2 \sigma^2} (z-\mu)^2 \Big) = 1? $$ \pause
\textcolor{blue}{Euler-Poisson Integral/Gaussian Integral} 
\end{itemize}
}


\frame{
\frametitle{\normalsize $N(0,1)$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=0, sigma=1, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $N(0,.5)$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=0, sigma=.5, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $N(0,.35)$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=0, sigma=.35, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $N(.5,.35)$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=.5, sigma=.35, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $N(-5,.35)$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=-.5, sigma=.35, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $N(0,.35)$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=0, sigma=.35, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\frame{
\frametitle{\normalsize $\textbf{N(0,1)}$}
\begin{center}
\psset{yunit=4cm,xunit=2}
\begin{pspicture*}(-2,-.25) (2,1.25) 
\uput[25](-.25,.5){\footnotesize $.5$}
\uput[25](-2,-.2){\footnotesize $-4$}
\uput[25](1.8,-.2){\footnotesize $4$}
\psaxes[ticks=none, labels=none]{->}(0,0)(-2.5,0)(2.5,1.25)
\psGauss[mue=0, sigma=1, linecolor=red, linewidth=2pt]{-2}{2}
\end{pspicture*} 
\end{center}
}

\section{Moments}




\frame{
\begin{center}
\textbf{Mean ($\mu$) and variance ($\sigma^2$)}
\end{center}
}

\frame{
\frametitle{\normalsize \scshape{Definiton: Mean of a discrete r.v.}}
\begin{itemize}[<+-|alert@+>]
\item [$\star$] {Let $X$ be a discrete r.v. with ($S$, $\prob_{X}$)} \vspace{.3cm}
\item [$\star$] {The mean or expected value of $X$ is defined as:}
$$\expec_{\prob_{X}}[X] \equiv \mu \equiv \sum_{x_n \in S} x_n \: \prob_{X}[X = x_n ] $$ 
\item [$\star$] {The variance of $X$ is defined as:}
$$\expec_{\prob_{X}}[(X-\mu)^2] \equiv \sigma^2 \equiv \sum_{x_n \in S} (x_n-\mu)^2 \: \prob_{X}[X = x_n ] $$ 

\end{itemize}
}

\frame{
\begin{center}
Let's compute the $\mu$ and $\sigma^2$ for some of the discrete distributions we have introduced 
\end{center}
}

\frame{
\frametitle{\normalsize \scshape {Bernoulli (p)} }
\begin{itemize}[<+- | alert@+>]
\item [$\star$] {S=\{0,1\}} \vspace{.3cm}
\item [$\star$] {$\expec_{\prob_{X}}[X] \equiv \mu = (0)(1-p) + 1 (p) = p$} \vspace{.3cm}
\item [$\star$] {$\expec_{\prob_{X}}[(X-\mu)^2] \equiv \sigma^2$ =} \pause
\textcolor{blue}{\begin{eqnarray*}
(0-p)^2(1-p) + (1-p)^2 p &=& p(1-p) [p + (1-p)] \\
&=& p(1-p) 
\end{eqnarray*} }  
\end{itemize}
}

\frame{
\frametitle{\normalsize \scshape {Binomial (n,p)}}
\begin{itemize}[<+- | alert@+>]
\item [$\star$] {S=\{0,1, \ldots n\}} \vspace{.3cm}
\item [$\star$] {$\expec_{\prob_{X}}[X] \equiv \mu = \ldots$}
\begin{eqnarray*}
\sum_{m=0}^{n} \frac{m n! p^m(1-p)^{n-m}}{(n-m)! m!}  &=& \sum_{m=1}^{n} \frac{n! p^m(1-p)^{n-m}}{(n-m)! m-1!} \\ \pause
&=& np \sum_{m=1}^{n} \frac{n-1! p^{m-1}(1-p)^{n-m}}{(n-m)! m-1!} \\ \pause
&=& \textcolor{blue}{np} \pause
\end{eqnarray*} 
\item [$\star$] {$\expec_{\prob_{X}}[(X-\mu)^2] \equiv \sigma^2$ =$n p(1-p)$} 
\end{itemize}
}


\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.1)$ } $\mu=.3, \sigma^2 = .27$}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(0,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.1}
\psline[linecolor=red](.3,-.05)(.3,.05)
\end{pspicture}
\end{center}
}


\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.3)$ }, $\mu=.9, \sigma^2 = .63 $}

\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.3}
\psline[linecolor=red](.9,-.05)(.9,.05)
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.5)$ }, $\mu=1.5, \sigma^2 = .75 $}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.5}
\psline[linecolor=red](1.5,-.05)(1.5,.05)
\end{pspicture}
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape Binomial $(3,.7)$, $\mu=2.1, \sigma^2 = .63 $}}
\begin{center}
\psset{xunit=1cm,yunit=5cm}
\begin{pspicture}(-1,.5)(7,0.55)
\psaxes[Dy=0.2,dy=0.2\psyunit]{->}(0,0)(-.5,0)(4,0)
\uput[-90](4,0){$\text{Supp}$} \uput[90](0,0.5){}
\psBinomial[markZeros,printValue,fillstyle=vlines]{3}{0.7}
\psline[linecolor=red](2.1,-.05)(2.1,.05)
\end{pspicture}
\end{center}
}

\frame{ 
\frametitle{\normalsize {\scshape Expected Value-Continuous Type}}
\begin{itemize} [<+->] 
\item [$\star$] For a discrete random variable $X$ we have defined the \textbf{expected value of a transformation:} 
$$ \sum_{i=1}^{n} u(x_i) \prob_X(X=x_i) $$
\item [$\star$] The analogous definition for a continuous-type random variable $X$ with p.d.f. $f_X$is:
\alert<2>{$$\expec_{f_X}[X] = \int_{-\infty}^{\infty} u(z) f_X(z) dz $$}
\end{itemize}
}

\frame{ 
\frametitle{\normalsize {$\mu$ and $\sigma$}}
\begin{itemize}
\item [$\star$] Therefore, the mean and the variance of a continuous-type r.v. X are given by:
\begin{eqnarray*}
\mu &=& \int_{-\infty}^{\infty} z f_X(z)dz \\
\sigma^2 &=& \int_{-\infty}^{\infty} (z-\mu)^2 f_X(z) dz
\end{eqnarray*}
\end{itemize}
}

\frame{
\begin{center}
Let's compute $\mu$ and $\sigma^2$ for the uniform $[a,b]$
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape U[a,b]}: $\mu$}
\begin{itemize}
\item [$\star$] $f(z)=\frac{1}{b-a} \mathbf{1}\{z \in [a,b]\}$ \vspace{.3cm}
\item [$\star$] $\mu$ is given by:
\begin{eqnarray*}
\mu = \int_{-\infty}^{\infty} z f(z) dz  &=& \frac{1}{b-a}\int_{a}^{b} z dz \\ \pause
&=& \frac{1}{2}\frac{1}{b-a} z^2 \Big]_{a}^{b} \\ \pause
&=& \frac{1}{2} \frac{1}{b-a} (b^2-a^2) \\ \pause
&=& \frac{b+a}{2}
\end{eqnarray*}
\end{itemize}
}

\frame{
\frametitle{\normalsize {\scshape U[a,b]}: $\sigma^2$}
\begin{itemize}
\item [$\star$] Note that 
$$\expec_f[(X-\mu)^2] = \expec_{f}[X^2] - 2\mu \expec_f[X] + \mu^2  $$ \pause
\item [$\star$] $\sigma^2$ is given by: 
\begin{eqnarray*}
\expec[X^2] = \int_{-\infty}^{\infty} z^2 f(z) dz  &=& \frac{1}{b-a}\int_{a}^{b} z^2 dz \\ \pause
&=& \frac{1}{3}\frac{1}{b-a} z^3 \Big]_{a}^{b} \\ \pause
&=& \frac{1}{3} \frac{1}{b-a} (b^3-a^3) \\ \pause
&=& \frac{b^2 + ab + a^2}{3}
\end{eqnarray*}
\end{itemize}
}

\frame{
\noindent Hence, $\sigma^2$ is given by:
$$ \frac{b^2 + ab + a^2}{3} - \frac{b^2 + 2ab + a^2}{4} = \frac{(b-a)^2}{12} $$
}


\frame{
\frametitle{\normalsize \scshape U$[a,b]: \mu$ }
\begin{center}
\begin{pspicture*}(-4,-4) (4,4) 
\psaxes[ticks=none, labels=none]{->}(0,-3)(-4,-3)(4,1.3)
\uput[25](-3.5,2){\footnotesize $f(z) = 
\left \{
\begin{array}{ccc}
0  & \text{if}  & z < a   \\
& &\\
1/(b-a) &  \text{if} & z \in [a,b]  \\
 & &\\
 0 & \text{if}  & z > b   
\end{array}
\right.
 $}
\uput[25](3.5,-3.4){\footnotesize $\R$}
\uput[25](-.8,-3.4){\footnotesize $\mu=\frac{a+b}{2}$}
\uput[25](-1.2,-3.4){\footnotesize $a$}
\uput[25](.8,-3.4){\footnotesize $b$}
\uput[25](1.2,-1.1){\footnotesize $1/(b-a)$}
\psline[linecolor=blue, linewidth=.1] (-4,-3)(-1,-3)
\psline[linecolor=blue, linewidth=.1] (1,-3)(4,-3)
\psPolynomial[coeff=-1 0, linewidth=.1, linecolor=blue]{-1}{1}
\end{pspicture*} 
\end{center}
}

\frame{
\begin{center}
Let's now compute the expectation and variance of the normal distribution
\end{center}
}

\frame{
\frametitle{\normalsize {\scshape $\mathcal{N}(\mu, \sigma^2)$}}
$$f(z) = \frac{1}{\sigma \sqrt{2 \pi}} \exp\Big(-\frac{1}{2\sigma^2} (z-\mu)^2 \Big) $$ \pause
Note first that:
$$ \int_{-\infty}^{\infty} z \frac{1}{\sigma \sqrt{2 \pi}} \exp\Big(-\frac{1}{2\sigma^2} z^2 \Big)dz = 0 \: \: \text{(why?)}  $$
}

\frame{
\frametitle{\normalsize {\scshape $\mathcal{N}(\mu, \sigma^2)$}}
Let
$$u= z-\mu $$ \pause
Note that:
\begin{eqnarray*}
\int_{-\infty}^{\infty} z \frac{1}{\sigma \sqrt{2 \pi}} \exp\Big(-\frac{1}{2\sigma^2} (z-\mu)^2 \Big) dz &=& \\ \pause
&& \\
\int_{-\infty}^{\infty} (u+\mu) \frac{1}{\sigma \sqrt{2 \pi}} \exp\Big(-\frac{1}{2\sigma^2} u^2 \Big) du &=& \mu 
\end{eqnarray*}
}


\frame{
\begin{center}
\textbf{Moment Generating Function}
\end{center}
}


\frame{
\frametitle{\normalsize \caps{MGF}}
\begin{itemize}[<+-|alert@+>]
\justifying
\item [$\star$] The real-valued random variable $X$ is said to have a moment generating function $m_{X}:(-\epsilon,\epsilon)\rightarrow [0,\infty]$ if
$$ E_{F}\Big[\exp \Big( t X \Big)\Big] < \infty, \quad \forall \: t \in (-\epsilon,\epsilon).$$
\item [$\star$] If the MGF is differentiable at $t=0$ we can obtain the k-th moment of $X$ as the $k$-th derivative of the MGF evaluated at $t=0$. 
\end{itemize}
}

\frame{
\frametitle{\normalsize \caps{Example}}
\begin{itemize}[<+-|alert@+>]
\item [$\star$] $X \sim \text{Bernoulli}(p)$
\item [] 
\begin{eqnarray*}
m_{X}(t) &=& E_{F}\Big[\exp \Big( t X \Big)\Big]  \\
&=& p \exp(t) + (1-p) \exp(0) \\
\end{eqnarray*}
\item [$\star$] Moments:
\item [] $ E_{F}[X]= p$ \\
\item [] $E_{F}[X^2] = p $ \\
\item [] $\vdots$
\item [] $E_{F}[X^k]=p$
\end{itemize}
}

\end{document}